import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import pandas as pd
import os
os.chdir('d:/My/Ê∏ÖÂçéÂ§ßÂ≠¶/Â≠¶‰π†/4.2Â§ßÂõõ‰∏ã/2ÊØï‰∏öËÆæËÆ°/MAPCSS')

import time

start_time = time.time()
df = pd.read_csv("./train_FD001_with_RUL.csv").iloc[:100]
test = pd.read_csv("./test_FD001_with_RUL.csv")

from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

def build_engine_id_map(df):
    engine_ids = sorted(df['engine_id'].unique())
    return {eid: idx for idx, eid in enumerate(engine_ids)}

def apply_engine_id_encoding(df, engine_id_map):
    df['engine_id_encoded'] = df['engine_id'].map(engine_id_map)
    df = df.dropna(subset=['engine_id_encoded']).copy()
    df['engine_id_encoded'] = df['engine_id_encoded'].astype(int)
    return df

def fit_scalers(df, feature_cols, label_col='RUL'):
    X_scaler = StandardScaler()
    y_scaler = StandardScaler()
    X_scaler.fit(df[feature_cols])
    y_scaler.fit(df[[label_col]])
    return X_scaler, y_scaler

def normalize_df(df, X_scaler, y_scaler, feature_cols, label_col='RUL'):
    df[feature_cols] = X_scaler.transform(df[feature_cols])
    df[label_col] = y_scaler.transform(df[[label_col]])
    return df

def generate_sequences(df, seq_len, feature_cols):
    X, y, engine_ids = [], [], []
    for eid, group in df.groupby('engine_id'):
        group = group.sort_values('cycle').reset_index(drop=True)
        if len(group) < seq_len or 'engine_id_encoded' not in group.columns:
            continue
        eid_encoded = group['engine_id_encoded'].iloc[0]
        for i in range(len(group) - seq_len + 1):
            seq = group.loc[i:i+seq_len-1, feature_cols].values
            label = group.loc[i+seq_len-1, 'RUL']
            X.append(seq)
            y.append(label)
            engine_ids.append(eid_encoded)
    return np.array(X), np.array(y), np.array(engine_ids)

def train_model(model, train_loader, epochs=20, lr=1e-3):
    model.train()
    # criterion = torch.nn.MSELoss()
    criterion = torch.nn.L1Loss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_list = []

    for epoch in range(epochs):
        total_loss = 0.0
        for xb, engine_ids, yb in train_loader:
            optimizer.zero_grad()
            preds = model(xb, engine_ids)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        loss_list.append(total_loss)
        print(f"[Epoch {epoch+1}/{epochs}] Loss: {total_loss:.4f}")
    return loss_list

def evaluate_model(model, test_loader, y_scaler):
    model.eval()
    all_preds, all_truths = [], []

    with torch.no_grad():
        for xb, engine_ids, yb in test_loader:
            preds = model(xb, engine_ids)
            all_preds.append(preds.numpy())
            all_truths.append(yb.numpy())

    all_preds = np.concatenate(all_preds).reshape(-1, 1)
    all_truths = np.concatenate(all_truths).reshape(-1, 1)

    # üîÅ ÂèçÂΩí‰∏ÄÂåñ
    all_preds_inv = y_scaler.inverse_transform(all_preds).flatten()
    all_truths_inv = y_scaler.inverse_transform(all_truths).flatten()

    mae = mean_absolute_error(all_truths_inv, all_preds_inv)
    mse = mean_squared_error(all_truths_inv, all_preds_inv)
    mean_pred = np.mean(all_preds_inv)
    var_pred = np.var(all_preds_inv)

    print(f"\nüìä Evaluation on Test Set:")
    print(f"MAE: {mae:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"Mean prediction: {mean_pred:.4f}")
    print(f"Variance of prediction: {var_pred:.4f}")

    return all_preds_inv, all_truths_inv

def plot_predictions(y_true, y_pred, title="True vs Predicted RUL"):
    plt.figure(figsize=(8, 6))
    plt.plot(y_true, label="True RUL", alpha=0.8)
    plt.plot(y_pred, label="Predicted RUL", alpha=0.8)
    plt.xlabel("Sample Index")
    plt.ylabel("RUL")
    plt.title(title)
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()

# ‚úÖ ÁîüÊàê engine_id ÁºñÁ†ÅÊò†Â∞ÑÔºà‰ªÖÁî®ËÆ≠ÁªÉÈõÜÔºâ
def build_engine_id_map(df):
    engine_ids = sorted(df['engine_id'].unique())
    return {eid: idx for idx, eid in enumerate(engine_ids)}

# ‚úÖ Â∫îÁî®Êò†Â∞ÑÂà∞ df ‰∏≠
def apply_engine_id_encoding(df, engine_id_map):
    df['engine_id_encoded'] = df['engine_id'].map(engine_id_map)
    df = df.dropna(subset=['engine_id_encoded']).copy()
    df['engine_id_encoded'] = df['engine_id_encoded'].astype(int)
    return df

# ‚úÖ Â∫èÂàóÁîüÊàêÂáΩÊï∞ÔºàËÆ≠ÁªÉ & ÊµãËØïÈÄöÁî®Ôºâ
def generate_sequences(df, seq_len, feature_cols):
    X, y, engine_ids = [], [], []

    for eid, group in df.groupby('engine_id'):
        group = group.sort_values('cycle').reset_index(drop=True)
        if len(group) < seq_len or 'engine_id_encoded' not in group.columns:
            continue
        eid_encoded = group['engine_id_encoded'].iloc[0]
        for i in range(len(group) - seq_len + 1):
            seq = group.loc[i:i+seq_len-1, feature_cols].values
            label = group.loc[i+seq_len-1, 'RUL']
            X.append(seq)
            y.append(label)
            engine_ids.append(eid_encoded)
    return np.array(X), np.array(y), np.array(engine_ids)

# ‚úÖ Ê®°ÂûãËÆ≠ÁªÉÂáΩÊï∞
def train_model(model, train_loader, epochs=20, lr=1e-3):
    model.train()
    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_list = []

    for epoch in range(epochs):
        total_loss = 0.0
        for xb, engine_ids, yb in train_loader:
            optimizer.zero_grad()
            preds = model(xb, engine_ids)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        loss_list.append(total_loss)
        print(f"[Epoch {epoch+1}/{epochs}] Loss: {total_loss:.4f}")
    return loss_list

# ‚úÖ Ê®°ÂûãËØÑ‰º∞ÂáΩÊï∞
def evaluate_model(model, test_loader, y_scaler):  # ‚úÖ Âä†‰∏ä y_scaler
    model.eval()
    all_preds, all_truths = [], []

    with torch.no_grad():
        for xb, engine_ids, yb in test_loader:
            preds = model(xb, engine_ids)
            all_preds.append(preds.numpy())
            all_truths.append(yb.numpy())

    all_preds = np.concatenate(all_preds).reshape(-1, 1)
    all_truths = np.concatenate(all_truths).reshape(-1, 1)

    # üîÅ ÂèçÂΩí‰∏ÄÂåñ
    all_preds_inv = y_scaler.inverse_transform(all_preds).flatten()
    all_truths_inv = y_scaler.inverse_transform(all_truths).flatten()

    mae = mean_absolute_error(all_truths_inv, all_preds_inv)
    mse = mean_squared_error(all_truths_inv, all_preds_inv)
    mean_pred = np.mean(all_preds_inv)
    var_pred = np.var(all_preds_inv)

    print(f"\nüìä Evaluation on Test Set:")
    print(f"MAE: {mae:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"Mean prediction: {mean_pred:.4f}")
    print(f"Variance of prediction: {var_pred:.4f}")

    return all_preds_inv, all_truths_inv

class LSTM_RUL_with_EngineEmbedding(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_engines, emb_dim=4):
        super().__init__()
        self.engine_embed = nn.Embedding(num_engines, emb_dim)
        self.lstm = nn.LSTM(input_size + emb_dim, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x, engine_ids):
        batch_size, seq_len, _ = x.size()
        emb = self.engine_embed(engine_ids).unsqueeze(1).expand(-1, seq_len, -1)
        x = torch.cat([x, emb], dim=-1)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out.squeeze()

# ‚úÖ ÂèØËßÜÂåñÂáΩÊï∞
def plot_predictions(y_true, y_pred, title="True vs Predicted RUL"):
    plt.figure(figsize=(8, 6))
    plt.plot(y_true, label="True RUL", alpha=0.8)
    plt.plot(y_pred, label="Predicted RUL", alpha=0.8)
    plt.xlabel("Sample Index")
    plt.ylabel("RUL")
    plt.title(title)
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()

# ËÆæÁΩÆÂèÇÊï∞
feature_cols = [f's{i}' for i in range(1, 22)]
seq_len = 15

# ÁîüÊàêÁºñÁ†Å
engine_id_map = build_engine_id_map(df)
df = apply_engine_id_encoding(df, engine_id_map)
test = apply_engine_id_encoding(test, engine_id_map)

# üéØ ÂΩí‰∏ÄÂåñÂô®ÊãüÂêà & Â∫îÁî®
X_scaler, y_scaler = fit_scalers(df, feature_cols)
df = normalize_df(df, X_scaler, y_scaler, feature_cols)
test = normalize_df(test, X_scaler, y_scaler, feature_cols)

# Â∫èÂàóÊï∞ÊçÆÁîüÊàê
X_train, y_train, engine_id_train = generate_sequences(df, seq_len, feature_cols)
X_test, y_test, engine_id_test = generate_sequences(test, seq_len, feature_cols)

# ËΩ¨Êç¢‰∏∫ TensorDataset
train_loader = DataLoader(
    TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                  torch.tensor(engine_id_train, dtype=torch.int64),
                  torch.tensor(y_train, dtype=torch.float32)),
    batch_size=64, shuffle=True
)

test_loader = DataLoader(
    TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                  torch.tensor(engine_id_test, dtype=torch.int64),
                  torch.tensor(y_test, dtype=torch.float32)),
    batch_size=64, shuffle=False
)

# ÂàùÂßãÂåñÊ®°Âûã
model = LSTM_RUL_with_EngineEmbedding(
    input_size=len(feature_cols),
    hidden_size=128,         # ‚Üë Êõ¥Ê∑±ÈöêËóèÂ±Ç
    num_layers=3,            # ‚Üë Â¢ûÂä†Êó∂Èó¥ÊçïÊçâËÉΩÂäõ
    num_engines=len(engine_id_map),
    emb_dim=8                # ‚Üë ÊèêÈ´ò engine Ë°®ËææËÉΩÂäõ
)

# ËÆ≠ÁªÉ + ËØÑ‰º∞ + ÂèØËßÜÂåñ
# train_model(model, train_loader, epochs=20, lr=1e-3)
train_model(model, train_loader, epochs=50, lr=5e-4)
preds, truths = evaluate_model(model, test_loader, y_scaler)
plot_predictions(truths, preds)

end_time = time.time()
print("ËøêË°åÊó∂Èó¥‰∏∫Ôºö{:.5f} Áßí".format(end_time - start_time))
# import seaborn as sns
# sns.histplot(y_train, kde=True)
# plt.title("Distribution of Normalized RUL (y_train)")
# plt.show()

# sns.histplot(preds - truths, kde=True)
# plt.title("Prediction Error (Pred - True)")
# plt.xlabel("Error")
# plt.show()

# print("È¢ÑÊµãÂÄºËåÉÂõ¥:", preds.min(), preds.max())
# print("ÁúüÂÆûÂÄºËåÉÂõ¥:", truths.min(), truths.max())

# plt.plot(X_train[0,:,0], label='s1')
# plt.plot(X_train[0,:,1], label='s2')
# plt.legend()
# plt.title("Êüê‰∏ÄËÆ≠ÁªÉÊ†∑Êú¨ÁöÑ‰º†ÊÑüÂô®ÂèòÂåñ")
# plt.show()

# sns.histplot(y_train, kde=True)
# plt.title("ËÆ≠ÁªÉÈõÜ RUL ÂàÜÂ∏É")
# plt.show()

# overlap = set(df['engine_id']) & set(test['engine_id'])
# print("ÈáçÂ§ç engine_id Êï∞Èáè:", len(overlap))  # Â∫îËØ•ÊòØ 0

# ‚úÖ Ê∏ÖÁàΩÂº∫ÂÅ•ÁâàÔºöLSTM + Êó†ÈáçÂ§ç engine + Âè™ÂΩí‰∏ÄÂåñÁâπÂæÅ + ReLU Èò≤Ë¥üÈ¢ÑÊµã

# import pandas as pd
# import numpy as np
# import torch
# import torch.nn as nn
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import mean_absolute_error, mean_squared_error
# from torch.utils.data import DataLoader, TensorDataset
# import matplotlib.pyplot as plt
# import seaborn as sns

# # ==== 1. ËØªÂèñÊï∞ÊçÆ ====
# df = pd.read_csv("train_FD004_with_RUL.csv")

# # ==== 2. ÂàÜÂâ≤ËÆ≠ÁªÉ / ÊµãËØï ====
# unique_engines = sorted(df['engine_id'].unique())
# train_ids = unique_engines[:80]   # Ââç 80 ‰∏™ engine
# test_ids = unique_engines[80:]    # ÂêéÈù¢Áî®‰∫éÊµãËØï

# df_train = df[df['engine_id'].isin(train_ids)].copy()
# df_test = df[df['engine_id'].isin(test_ids)].copy()

# # ==== 3. ÁºñÁ†Å engine_id ====
# engine_id_map = {eid: idx for idx, eid in enumerate(train_ids)}
# df_train['engine_id_encoded'] = df_train['engine_id'].map(engine_id_map)
# df_test['engine_id_encoded'] = df_test['engine_id'].map(engine_id_map)
# df_test = df_test.dropna(subset=['engine_id_encoded']).copy()
# df_test['engine_id_encoded'] = df_test['engine_id_encoded'].astype(int)

# # ==== 4. ÁâπÂæÅÂΩí‰∏ÄÂåñÔºà‰∏çÂΩí‰∏ÄÂåñ RULÔºâ ====
# feature_cols = [f's{i}' for i in range(1, 22)]
# X_scaler = StandardScaler()
# df_train[feature_cols] = X_scaler.fit_transform(df_train[feature_cols])
# df_test[feature_cols] = X_scaler.transform(df_test[feature_cols])

# # ==== 5. ÊûÑÂª∫Êó∂Â∫èÊ†∑Êú¨ ====
# def generate_sequences(df, seq_len, feature_cols):
#     X, y, engine_ids = [], [], []
#     for eid, group in df.groupby('engine_id'):
#         group = group.sort_values('cycle').reset_index(drop=True)
#         if len(group) < seq_len:
#             continue
#         eid_encoded = group['engine_id_encoded'].iloc[0]
#         for i in range(len(group) - seq_len + 1):
#             seq = group.loc[i:i+seq_len-1, feature_cols].values
#             label = group.loc[i+seq_len-1, 'RUL']
#             X.append(seq)
#             y.append(label)
#             engine_ids.append(eid_encoded)
#     return np.array(X), np.array(y), np.array(engine_ids)

# seq_len = 15
# X_train, y_train, eid_train = generate_sequences(df_train, seq_len, feature_cols)
# X_test, y_test, eid_test = generate_sequences(df_test, seq_len, feature_cols)

# # ==== 6. ÊûÑÂª∫ DataLoader ====
# train_loader = DataLoader(TensorDataset(
#     torch.tensor(X_train, dtype=torch.float32),
#     torch.tensor(eid_train, dtype=torch.long),
#     torch.tensor(y_train, dtype=torch.float32)),
#     batch_size=64, shuffle=True
# )

# test_loader = DataLoader(TensorDataset(
#     torch.tensor(X_test, dtype=torch.float32),
#     torch.tensor(eid_test, dtype=torch.long),
#     torch.tensor(y_test, dtype=torch.float32)),
#     batch_size=64, shuffle=False
# )

# # ==== 7. Ê®°ÂûãÂÆö‰πâÔºàReLU Èò≤Ë¥ü RULÔºâ ====
# class LSTM_RUL_with_EngineEmbedding(nn.Module):
#     def __init__(self, input_size, hidden_size, num_layers, num_engines, emb_dim=8):
#         super().__init__()
#         self.engine_embed = nn.Embedding(num_engines, emb_dim)
#         self.lstm = nn.LSTM(input_size + emb_dim, hidden_size, num_layers, batch_first=True)
#         self.fc = nn.Sequential(
#             nn.Linear(hidden_size, 1),
#             nn.ReLU()
#         )

#     def forward(self, x, engine_ids):
#         emb = self.engine_embed(engine_ids).unsqueeze(1).expand(-1, x.size(1), -1)
#         x = torch.cat([x, emb], dim=-1)
#         out, _ = self.lstm(x)
#         out = self.fc(out[:, -1, :])
#         return out.squeeze()

# # ==== 8. ËÆ≠ÁªÉÂáΩÊï∞ ====
# def train_model(model, loader, epochs=30, lr=1e-3):
#     model.train()
#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#     loss_fn = nn.L1Loss()
#     for epoch in range(epochs):
#         total_loss = 0
#         for xb, eids, yb in loader:
#             pred = model(xb, eids)
#             loss = loss_fn(pred, yb)
#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()
#             total_loss += loss.item()
#         print(f"[Epoch {epoch+1}] Loss: {total_loss:.4f}")

# # ==== 9. ËØÑ‰º∞ÂáΩÊï∞ ====
# def evaluate_model(model, loader):
#     model.eval()
#     preds, truths = [], []
#     with torch.no_grad():
#         for xb, eids, yb in loader:
#             pred = model(xb, eids)
#             preds.append(pred.numpy())
#             truths.append(yb.numpy())
#     preds = np.concatenate(preds)
#     truths = np.concatenate(truths)
#     print("\nüìä Evaluation:")
#     print("MAE:", mean_absolute_error(truths, preds))
#     print("MSE:", mean_squared_error(truths, preds))
#     print("È¢ÑÊµãÂÄºËåÉÂõ¥:", preds.min(), preds.max())
#     print("ÁúüÂÆûÂÄºËåÉÂõ¥:", truths.min(), truths.max())
#     return preds, truths

# # ==== 10. ÁîªÂõæÂáΩÊï∞ ====
# def plot_predictions(y_true, y_pred):
#     plt.figure(figsize=(8, 6))
#     plt.plot(y_true, label='True RUL', alpha=0.8)
#     plt.plot(y_pred, label='Predicted RUL', alpha=0.8)
#     plt.legend()
#     plt.title("True vs Predicted RUL")
#     plt.xlabel("Sample Index")
#     plt.ylabel("RUL")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

# # ==== 11. ËøêË°åÂÖ®ÈÉ®ÊµÅÁ®ã ====
# model = LSTM_RUL_with_EngineEmbedding(
#     input_size=len(feature_cols),
#     hidden_size=128,
#     num_layers=2,
#     num_engines=len(engine_id_map),
#     emb_dim=8
# )

# train_model(model, train_loader, epochs=40, lr=5e-4)
# preds, truths = evaluate_model(model, test_loader)
# plot_predictions(truths, preds)

